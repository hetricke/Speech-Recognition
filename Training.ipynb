{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from tempfile import mktemp\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that were altered to test for best architecture\n",
    "EPOCHS =1\n",
    "BATCH_SIZE = 1\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "LATENT_DIM = 256\n",
    "\n",
    "#Globals\n",
    "target_characters = []\n",
    "image_name = 0\n",
    "x_processed_data = []\n",
    "y_processed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/hetricke/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/en/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631)\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "#dataset loaded from hugging face\n",
    "ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function used to remove silence from beginning and end of audio segments\n",
    "def detect_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):\n",
    "    trim_ms = 0\n",
    "\n",
    "    assert chunk_size > 0\n",
    "    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold and trim_ms < len(sound):\n",
    "        trim_ms += chunk_size\n",
    "\n",
    "    return trim_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regulates transcripts and converts it into a list of integers\n",
    "def transcript_prep(transcription):\n",
    "\n",
    "    global target_characters\n",
    "\n",
    "    #removes trailing quotation marks\n",
    "    if transcription.startswith('\"') and transcription.endswith('\"'):\n",
    "        transcription = transcription[1:-1]\n",
    "\n",
    "    transcription = transcription.lower()\n",
    "    \n",
    "    #adds a period to the end of sentence if it doesn't already end in punctuation\n",
    "    if len(transcription) > 0:\n",
    "        if transcription[-1] not in [\".\", \"?\", \"!\"]:\n",
    "            transcription = transcription + \".\"\n",
    "\n",
    "    for letter in transcription:\n",
    "        if letter not in target_characters:\n",
    "            target_characters.append(letter)\n",
    "\n",
    "    return list(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processes the audio file input for use in the neural network\n",
    "def audio_file_prep(audio_path, sentence_length):\n",
    "\n",
    "    global image_name\n",
    "\n",
    "    #reads in mp3\n",
    "    mp3_audio = AudioSegment.from_file(audio_path, format=\"mp3\")  # read mp3\n",
    "\n",
    "    #returns -1 if the mp3 file is empty\n",
    "    if(round(mp3_audio.duration_seconds) == 0):\n",
    "        return -1\n",
    "\n",
    "    #removes silent audio from the beginning and end\n",
    "    start_trim = detect_leading_silence(mp3_audio)\n",
    "    end_trim = detect_leading_silence(mp3_audio.reverse())\n",
    "    duration = len(mp3_audio)    \n",
    "    trimmed_sound = mp3_audio[start_trim:duration-end_trim]\n",
    "\n",
    "    #converts the mp3 into a wav file\n",
    "    wname = mktemp('.wav')\n",
    "    trimmed_sound.export(wname, format=\"wav\")\n",
    "    FS, audio_data = wavfile.read(wname)\n",
    "\n",
    "\n",
    "    #creates a file name for the spectrogram\n",
    "    file_name = \"images/\"+ str(image_name) + \".png\"\n",
    "    image_name = image_name + 1\n",
    "\n",
    "    #creates and saves the spectrogram\n",
    "    plt.figure()\n",
    "    plt.specgram(audio_data, Fs=FS, NFFT=128, noverlap=0)  # plot\n",
    "    plt.axis('off')\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "\n",
    "    #clears the figure for the next audio transcript- otherwise it just overwrites the image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #loads the spectrogram and turns it into an array\n",
    "    img = keras.preprocessing.image.load_img(file_name)\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    #flattens the array and normalizes the data\n",
    "    new_dim = img_array.shape[0]*img_array.shape[1]\n",
    "    img_array = img_array.reshape(new_dim, -1)\n",
    "    img_array = img_array.flatten()\n",
    "    img_array = img_array.tolist()\n",
    "    img_array[:] = [x / 255 for x in img_array]\n",
    "\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(beginning_datapoint, ending_datapoint): \n",
    "    #pre-processes all the data\n",
    "    global image_name\n",
    "    global x_processed_data\n",
    "    global y_processed_data\n",
    "    global target_characters\n",
    "\n",
    "    image_name = 0\n",
    "    x_processed_data = []\n",
    "    y_processed_data = []\n",
    "    \n",
    "    for i in range(ending_datapoint-beginning_datapoint):\n",
    "        x_processed_data.append(audio_file_prep(ds['train'][i+beginning_datapoint]['path'], len(ds['train'][i+beginning_datapoint][\"sentence\"])))\n",
    "        y_processed_data.append(transcript_prep(ds['train'][i][\"sentence\"]))\n",
    "\n",
    "\n",
    "        #if something about the data is bad, it is removed from the set\n",
    "        if(x_processed_data[len(x_processed_data)-1] == -1 or len(y_processed_data[len(y_processed_data)-1])==0):\n",
    "            x_processed_data.pop(len(x_processed_data)-1)\n",
    "            y_processed_data.pop(len(y_processed_data)-1)\n",
    "\n",
    "\n",
    "    target_characters = sorted(target_characters)\n",
    "    num_encoder_tokens = 256\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    max_encoder_seq_length = max(len(audio) for audio in x_processed_data)\n",
    "    max_decoder_seq_length = max(len(transcript) for transcript in y_processed_data)\n",
    "\n",
    "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "    encoder_input_data = np.zeros( (len(x_processed_data), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros( (len(y_processed_data), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "    decoder_target_data = np.zeros( (len(y_processed_data), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "    for i, (x_datapoint, y_datapoint) in enumerate(zip(x_processed_data, y_processed_data)):\n",
    "        for t, val in enumerate(x_datapoint):\n",
    "            #print(str(i)+\", \"+str(t)+\", \"+str(val))\n",
    "            encoder_input_data[i, t, int(val*255)] = 1.0\n",
    "        encoder_input_data[i, t + 1 :, 0] = 1.0\n",
    "        for t, char in enumerate(y_datapoint):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = keras.layers.LSTM(LATENT_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = keras.layers.LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "        [encoder_input_data, decoder_input_data],\n",
    "        decoder_target_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.2,\n",
    "    )\n",
    "    # Save model\n",
    "    model.save(\"s2s_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['invalidated', 'other', 'test', 'train', 'validation']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/hetricke/Documents/School/ECE 579 Intelligent Systems/Term Project/Code/Speech-Recognition/Training.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(ds),\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_model(data,data\u001b[39m+\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/hetricke/Documents/School/ECE 579 Intelligent Systems/Term Project/Code/Speech-Recognition/Training.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_processed_data \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ending_datapoint\u001b[39m-\u001b[39mbeginning_datapoint):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x_processed_data\u001b[39m.\u001b[39mappend(audio_file_prep(ds[i\u001b[39m+\u001b[39;49mbeginning_datapoint][\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mlen\u001b[39m(ds[i\u001b[39m+\u001b[39mbeginning_datapoint][\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     y_processed_data\u001b[39m.\u001b[39mappend(transcript_prep(ds[i][\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hetricke/Documents/School/ECE%20579%20Intelligent%20Systems/Term%20Project/Code/Speech-Recognition/Training.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m#if something about the data is bad, it is removed from the set\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/speech_recog_env/lib/python3.11/site-packages/datasets/dataset_dict.py:63\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m     split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     61\u001b[0m ]\n\u001b[1;32m     62\u001b[0m suggested_split \u001b[39m=\u001b[39m available_suggested_splits[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m available_suggested_splits \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m. Please first select a split. For example: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`my_dataset_dictionary[\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msuggested_split\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m][\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvailable splits: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['invalidated', 'other', 'test', 'train', 'validation']\""
     ]
    }
   ],
   "source": [
    "for data in range(0,len(ds),10):\n",
    "    train_model(data,data+10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 14:30:51.536683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 14:30:51.537798: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 14:30:51.539101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-12-06 14:30:51.754754: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 14:30:51.756946: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 14:30:51.758843: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-12-06 14:30:52.016690: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 14:30:52.018027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 14:30:52.019036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"s2s_model.keras\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = keras.Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    #target_seq[0, 0, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 14:41:29.212864: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 14:41:29.214818: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 14:41:29.217312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ['t', 'h', 'e', ' ', 't', 'r', 'a', 'c', 'k', ' ', 'a', 'p', 'p', 'e', 'a', 'r', 's', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'p', 'i', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'l', 'b', 'u', 'm', ' ', '\"', 'k', 'r', 'a', 'f', 't', 'w', 'o', 'r', 'k', 's', '\"', '.']\n",
      "Decoded sentence: hheett                                                                                       \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(1):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", y_processed_data[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
