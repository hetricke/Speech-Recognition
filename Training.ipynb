{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 11:18:47.121912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/hetricke/anaconda3/envs/speech_recog_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from tempfile import mktemp\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that were altered to test for best architecture\n",
    "TIME_STEPS = 5\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "LATENT_DIM = 256\n",
    "\n",
    "#Globals\n",
    "target_characters = []\n",
    "image_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/hetricke/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/en/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631)\n"
     ]
    }
   ],
   "source": [
    "#dataset loaded from hugging face\n",
    "ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split='train[:100]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function used to remove silence from beginning and end of audio segments\n",
    "def detect_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):\n",
    "    trim_ms = 0\n",
    "\n",
    "    assert chunk_size > 0\n",
    "    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold and trim_ms < len(sound):\n",
    "        trim_ms += chunk_size\n",
    "\n",
    "    return trim_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regulates transcripts and converts it into a list of integers\n",
    "def transcript_prep(transcription):\n",
    "\n",
    "    global target_characters\n",
    "\n",
    "    #removes trailing quotation marks\n",
    "    if transcription.startswith('\"') and transcription.endswith('\"'):\n",
    "        transcription = transcription[1:-1]\n",
    "\n",
    "    transcription = transcription.lower()\n",
    "    \n",
    "    #adds a period to the end of sentence if it doesn't already end in punctuation\n",
    "    if len(transcription) > 0:\n",
    "        if transcription[-1] not in [\".\", \"?\", \"!\"]:\n",
    "            transcription = transcription + \".\"\n",
    "\n",
    "    for letter in transcription:\n",
    "        if letter not in target_characters:\n",
    "            target_characters.append(letter)\n",
    "\n",
    "    return list(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processes the audio file input for use in the neural network\n",
    "def audio_file_prep(audio_path, sentence_length):\n",
    "\n",
    "    global image_name\n",
    "\n",
    "    #reads in mp3\n",
    "    mp3_audio = AudioSegment.from_file(audio_path, format=\"mp3\")  # read mp3\n",
    "\n",
    "    #returns -1 if the mp3 file is empty\n",
    "    if(round(mp3_audio.duration_seconds) == 0):\n",
    "        return -1\n",
    "\n",
    "    #removes silent audio from the beginning and end\n",
    "    start_trim = detect_leading_silence(mp3_audio)\n",
    "    end_trim = detect_leading_silence(mp3_audio.reverse())\n",
    "    duration = len(mp3_audio)    \n",
    "    trimmed_sound = mp3_audio[start_trim:duration-end_trim]\n",
    "\n",
    "    #converts the mp3 into a wav file\n",
    "    wname = mktemp('.wav')\n",
    "    trimmed_sound.export(wname, format=\"wav\")\n",
    "    FS, audio_data = wavfile.read(wname)\n",
    "\n",
    "\n",
    "    #creates a file name for the spectrogram\n",
    "    file_name = \"images/\"+ str(image_name) + \".png\"\n",
    "    image_name = image_name + 1\n",
    "\n",
    "    #creates and saves the spectrogram\n",
    "    plt.figure()\n",
    "    plt.specgram(audio_data, Fs=FS, NFFT=128, noverlap=0)  # plot\n",
    "    plt.axis('off')\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "\n",
    "    #clears the figure for the next audio transcript- otherwise it just overwrites the image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #loads the spectrogram and turns it into an array\n",
    "    img = keras.preprocessing.image.load_img(file_name)\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "\n",
    "    new_dim = img_array.shape[0]*img_array.shape[1]\n",
    "    img_array = img_array.reshape(new_dim, -1)\n",
    "    img_array = img_array.flatten()\n",
    "    img_array = img_array.tolist()\n",
    "    img_array[:] = [x / 255 for x in img_array]\n",
    "\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 0\n",
    "x_processed_data = []\n",
    "y_processed_data = []\n",
    "\n",
    "#pre-processes all the data\n",
    "for i in range(25):\n",
    "\n",
    "    x_processed_data.append(audio_file_prep(ds[i]['path'], len(ds[i][\"sentence\"])))\n",
    "    y_processed_data.append(transcript_prep(ds[i][\"sentence\"]))\n",
    "\n",
    "    #if something about the data is bad, it is removed from the set\n",
    "    if(x_processed_data[len(x_processed_data)-1] == -1 or len(y_processed_data[len(y_processed_data)-1])==0):\n",
    "       x_processed_data.pop(len(x_processed_data)-1)\n",
    "       y_processed_data.pop(len(y_processed_data)-1)\n",
    "\n",
    "\n",
    "target_characters = sorted(target_characters)\n",
    "num_encoder_tokens = 256\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max(len(audio) for audio in x_processed_data)\n",
    "max_decoder_seq_length = max(len(transcript) for transcript in y_processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros( (len(x_processed_data), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "decoder_input_data = np.zeros( (len(y_processed_data), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "decoder_target_data = np.zeros( (len(y_processed_data), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "for i, (x_datapoint, y_datapoint) in enumerate(zip(x_processed_data, y_processed_data)):\n",
    "    for t, val in enumerate(x_datapoint):\n",
    "        #print(str(i)+\", \"+str(t)+\", \"+str(val))\n",
    "        encoder_input_data[i, t, int(val*255)] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, 0] = 1.0\n",
    "    for t, char in enumerate(y_datapoint):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 11:21:43.667752: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-06 11:21:44.787637: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 11:21:44.789905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 11:21:44.791951: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-12-06 11:21:45.137020: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-06 11:21:45.139388: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-06 11:21:45.141382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/60 [==>...........................] - ETA: 1s - loss: 3.6272 - accuracy: 0.1667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:58:31.432894: W tensorflow/core/grappler/utils/graph_view.cc:849] No registered '' OpKernel for CPU devices compatible with node {{node sequential/simple_rnn/while/body/_1/sequential/simple_rnn/while/simple_rnn_cell/Relu}}\n",
      "\t.  Registered:  <no registered kernels>\n",
      "\n",
      "2023-12-05 21:58:31.440680: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{any(tfg-consolidate-attrs,tfg-toposort,tfg-shape-inference{graph-version=0},tfg-prepare-attrs-export)} failed: INVALID_ARGUMENT: Node sequential/simple_rnn/while/body/_1/sequential/simple_rnn/while/simple_rnn_cell/Relu has an empty op name\n",
      "\twhen importing GraphDef to MLIR module in GrapplerHook\n",
      "2023-12-05 21:58:31.450057: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{any(tfg-consolidate-attrs,tfg-functional-to-region,tfg.func(tfg-cf-sink),tfg-region-to-functional{force-control-capture=true},tfg-lift-legacy-call,symbol-privatize{},symbol-dce,tfg-prepare-attrs-export)} failed: INVALID_ARGUMENT: Node sequential/simple_rnn/while/body/_1/sequential/simple_rnn/while/simple_rnn_cell/Relu has an empty op name\n",
      "\twhen importing GraphDef to MLIR module in GrapplerHook\n",
      "2023-12-05 21:58:31.450458: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{any(tfg-consolidate-attrs,tfg-functional-to-region,tfg.func(tfg-cf-sink),tfg-region-to-functional{force-control-capture=true},tfg-lift-legacy-call,symbol-privatize{},symbol-dce,tfg-prepare-attrs-export)} failed: INVALID_ARGUMENT: Node sequential/simple_rnn/while/body/_1/sequential/simple_rnn/while/simple_rnn_cell/Relu has an empty op name\n",
      "\twhen importing GraphDef to MLIR module in GrapplerHook\n",
      "2023-12-05 21:58:31.450978: W tensorflow/core/common_runtime/optimize_function_graph_utils.cc:475] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Node 'sequential/simple_rnn/while/body/_1/sequential/simple_rnn/while/simple_rnn_cell/Relu' does not specify an operation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 24ms/step - loss: 3.1118 - accuracy: 0.1500\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 3.7493 - accuracy: 0.0795\n",
      "65/65 [==============================] - 1s 22ms/step - loss: 2.9959 - accuracy: 0.0923\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.8015 - accuracy: 0.1458\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 3.2088 - accuracy: 0.1268\n",
      "0.11888940781354904\n"
     ]
    }
   ],
   "source": [
    "#tests the accurracy of the generated model\n",
    "total_score = 0\n",
    "total_accuracy = 0\n",
    "for i in range(len(x_test)):\n",
    "    score, accuracy = model.evaluate(x_test[i], y_test[i], batch_size=BATCH_SIZE)\n",
    "    total_score += score\n",
    "    total_accuracy += accuracy\n",
    "\n",
    "total_score /= len(x_test)\n",
    "total_accuracy /= len(x_test)\n",
    "\n",
    "print(total_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
